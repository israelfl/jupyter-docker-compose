{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73ef6c8-c48b-4fe5-bbeb-7186192fe2ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Tutorial de LangChain con Google Gemini\n",
    "\n",
    "Este tutorial te guiará a través del uso de LangChain con el modelo Gemini de Google. Aprenderás desde los  conceptos básicos hasta implementaciones avanzadas.\n",
    "\n",
    "## Contenido\n",
    "1. Introducción a LangChain\n",
    "2. Configuración del entorno\n",
    "3. Ejemplos básicos\n",
    "4. Casos de uso prácticos\n",
    "5. Integraciones avanzadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5642c362-0673-4576-97f4-153026350500",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. Introducción a LangChain\n",
    "\n",
    "LangChain es un framework que facilita la creación de aplicaciones usando modelos de lenguaje (LLMs). Proporciona abstracciones útlies para:\n",
    "- Encadenar llamadas a LLMs\n",
    "- Integrar con fuentes de datos externas\n",
    "- Crear agentes interactivos\n",
    "- Implementar patrones comunes de uso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d288d5-962d-4a72-ba0b-ad58b76146d5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. Configuración del entorno\n",
    "\n",
    "Primeros, instalemos las dependencias necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdbefb5-e75f-45b9-9fc4-c697d9a79681",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain google-generativeai python-dotenv langchain_community langchain-google-genai faiss-cpu -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577678d-50e2-4627-9614-80a34b3aff86",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Ahora, importemos las bibliotecas necesarias y configuraremos el entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b32e56-b58f-4c91-a08e-9cbf28955fce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec1320a-9562-4af6-add7-04cf8649ae5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Configurar el modelo Gemini\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    google_api_key=os.getenv(\"GENAI_API_KEY\"),\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1379e896-1d21-46e6-8ffc-b3ee64fc60c9",
   "metadata": {},
   "source": [
    "## 3. Ejemplos Básicos\n",
    "### 3.1 Conversación Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81807af2-e8a0-45f0-8e3b-652f11d7c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo básico de chat\n",
    "#response = llm.invoke(\"¿Cuáles son los principales beneficios de usar LangChain?\")\n",
    "response = llm.invoke(\"¿Qué es LangChain?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbf86f-82f4-45d7-9f8c-8cbb983492da",
   "metadata": {},
   "source": [
    "### 3.2 Uso de Plantillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281463ad-5160-4176-910c-09934da55e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una plantilla de prompt\n",
    "template = \"\"\"Actúa como un experto en {tema}.\n",
    "Proporciona una explicación detallada sobre: {concepto}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62500c-96a1-4eed-a2cc-4115903cd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"tema\", \"concepto\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74118e3e-4f28-4b74-8393-9e0870917850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una cadena\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de0f42-127e-4905-a5bd-51a218a9a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eejcutar la cadena\n",
    "response = chain.invoke({\n",
    "    \"tema\": \"inteligencia attificial\",\n",
    "    \"concepto\": \"redes neuronales convolucionales\"\n",
    "})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb25da-0fb9-49e0-badf-509bb9bf6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d1322d-e628-4ac3-93bd-3cd99813809d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Configurar memoria\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a4e04a-797b-443b-827f-16e2a7e8a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear plantilla para el chat\n",
    "template = \"\"\"La siguiente es una conversación amigable entre un humano y un asistente de AI.\n",
    "Historial de la conversación:\n",
    "{chat_history}\n",
    "Humano: {input}\n",
    "Asistente:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860dc25-6762-4724-8af8-2b2782dcc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"input\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459da52f-2ea3-4232-8d21-7310feb768d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la cadena con memoria y plantilla\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6234afe-b4a4-4423-81a3-57f77a198da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.predict(input=\"¿Qué país ha ganado más mundiales de futbol?\"))\n",
    "print(conversation.predict(input=\"Nómbrame 10 jugadores históricos de esa selección\"))\n",
    "print(conversation.predict(input=\"¿De qué color es su camiseta?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e2594-6640-48aa-9281-fe4e00bb3774",
   "metadata": {},
   "source": [
    "## 4. Análisis de Sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c199f-602f-4a04-b366-b0b56cc511a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Crear plantilla para análisis de sentimientos\n",
    "sentiment_template = \"\"\"Analiza el sentimiento del siguiente texto y clasifícalo como positivo\n",
    "negativo o neutral. Proporciona también una explicación de tu análisis.\n",
    "\n",
    "Texto {texto}\n",
    "\n",
    "Formato de respuesta:\n",
    "Sentimiento: [clasificación]\n",
    "Explicación [tu análisis]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20926784-e42a-43d8-9036-6679cd65a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_prompt = PromptTemplate(\n",
    "    input_variables=[\"texto\"],\n",
    "    template=sentiment_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320f35b-99a9-4baa-9615-41e374d92f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la cadena\n",
    "sentiment_chain = LLMChain(llm=llm, prompt=sentiment_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a743a-51d0-45d0-a1c2-db3a22f403e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de análisis\n",
    "texto_ejemplo = \"El nuevo restaurante superó todas mis expectativas. La comida estaba deliciosa y el servicio fue excelente.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9059f59-346f-415b-9705-4b78c4480dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar el método recomendado 'predict'\n",
    "resultado = sentiment_chain.predict(texto=texto_ejemplo)\n",
    "\n",
    "# Imprimir resultado\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9a2b1-e065-4117-a41c-55d1587b70e8",
   "metadata": {},
   "source": [
    "## 5. Integraciones Avanzadas\n",
    "### 5.1 Integración con Fuentes de Datos Externas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eccec3c-0c85-49b5-b42d-d90045bd44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb153d1-a998-4e59-b926-50830b38e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ejemplo.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\"\"LangChain es un framework de código abierto diseñado para facilitar el desarrollo de aplicaciones impulsadas por modelos de lenguaje (LLMs). En esencia, LangChain proporciona las herramientas y la infraestructura necesarias para conectar LLMs con otras fuentes de datos y herramientas externas, permitiendo crear aplicaciones más sofisticadas y con mayor funcionalidad.\n",
    "\n",
    "Aquí te desgloso los puntos clave de LangChain:\n",
    "\n",
    "Propósito Principal:\n",
    "\n",
    "Facilitar el desarrollo de aplicaciones con LLMs: LangChain simplifica la integración de LLMs en diferentes tipos de aplicaciones, desde chatbots hasta sistemas de análisis de datos.\n",
    "Cadena de acciones (Chains): Permite construir \"cadenas\" de operaciones, donde la salida de un LLM se convierte en la entrada de otro o de una herramienta externa. Esto permite crear flujos de trabajo complejos y automatizados.\n",
    "Conexión con el mundo exterior: Permite a los LLMs interactuar con bases de datos, APIs, herramientas de búsqueda en la web y otros recursos externos, ampliando su capacidad y utilidad.\n",
    "Componentes Clave:\n",
    "\n",
    "Modelos: Integra una amplia variedad de modelos de lenguaje, incluyendo modelos de OpenAI (GPT-3, GPT-4), Hugging Face, Cohere, etc.\n",
    "Prompts (Instrucciones): Ofrece herramientas para la creación, gestión y optimización de prompts, que son fundamentales para obtener los resultados deseados de los LLMs.\n",
    "Índices (Indexes): Permite construir y gestionar índices de datos para que los LLMs puedan acceder a información específica de forma eficiente.\n",
    "Cadenas (Chains): Proporciona la lógica para conectar diferentes componentes y crear flujos de trabajo complejos.\n",
    "Agentes (Agents): Permite a los LLMs decidir qué acciones tomar en función de la información disponible, como por ejemplo, buscar en internet, consultar una base de datos, etc.\n",
    "Memorias (Memory): Permite a los LLMs recordar interacciones previas, lo que es crucial para construir aplicaciones conversacionales.\n",
    "Herramientas (Tools): Facilita la integración de herramientas externas, como APIs, bases de datos, etc.\n",
    "Beneficios de usar LangChain:\n",
    "\n",
    "Desarrollo más rápido: Simplifica el proceso de desarrollo al proporcionar componentes reutilizables y fáciles de integrar.\n",
    "Mayor flexibilidad: Permite construir aplicaciones más complejas y personalizadas.\n",
    "Acceso a más datos: Permite a los LLMs interactuar con fuentes de datos externas, lo que aumenta su capacidad de comprensión y respuesta.\n",
    "Mayor control: Ofrece más control sobre el comportamiento de los LLMs.\n",
    "Comunidad activa: Cuenta con una comunidad activa que contribuye al desarrollo y mejora del framework.\n",
    "Ejemplos de uso:\n",
    "\n",
    "Chatbots inteligentes: Crear chatbots que pueden responder preguntas, proporcionar información y realizar tareas específicas.\n",
    "Análisis de datos: Automatizar el análisis de grandes conjuntos de datos utilizando el poder del procesamiento del lenguaje natural.\n",
    "Generación de contenido: Crear contenido de texto, como artículos, correos electrónicos y descripciones de productos.\n",
    "Agentes automatizados: Construir agentes que puedan realizar tareas complejas de forma autónoma, como reservar vuelos, responder correos electrónicos o gestionar proyectos.\n",
    "Herramientas de búsqueda: Integrar LLMs con motores de búsqueda para obtener información más relevante y precisa.\n",
    "En resumen, LangChain es una herramienta poderosa para cualquier persona que quiera aprovechar el potencial de los modelos de lenguaje para construir aplicaciones innovadoras y funcionales. Proporciona una capa de abstracción que simplifica la interacción con los LLMs y permite conectarlos con el mundo exterior, abriendo un abanico de posibilidades para el desarrollo de nuevas aplicaciones.\n",
    "\n",
    "Si estás interesado en aprender más, te recomiendo explorar la documentación oficial de LangChain: https://python.langchain.com/\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d108e-ab35-42c8-86a0-d871e3ce7b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y procesar el documento\n",
    "loader = TextLoader('ejemplo.txt', encoding='utf-8')\n",
    "documents = loader.load()\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaeb065-6340-4c36-a3a0-788efaff9032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el texto en chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa4b5e-6a16-42c0-bca4-79bf542d211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668c1f20-b74e-44b0-be20-acbe827d779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=os.getenv(\"GENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be51b0c-5ca5-4f39-b001-04c7245d585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear base de datos vectorial\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0512a-fc19-4859-b7c1-266b73f022d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar una búsqueda\n",
    "query = \"¿Qué es LangChain?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a68901-36a5-425e-8027-a2d889a91f78",
   "metadata": {},
   "source": [
    "### 5.2 Creación de un Agente Personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c9ee4-84f7-4dac-948b-c250f0b1c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3910a9-72c0-4d41-a440-76d5d2814b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener la hora actual\n",
    "def get_current_time(_input=None): # Acepta un argumento pero no lo usa\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c03246-a179-4b3a-8a49-5ddf9432e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular la suma de números\n",
    "def calculate_sum(numbers_str):\n",
    "    try:\n",
    "        numbers = [float(n) for n in numbers_str.split()]\n",
    "        return str(sum(numbers))\n",
    "    except Exception as e:\n",
    "        return \"Error: Por favor proporciona números separados por espacios\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae1980-38d5-4af2-a353-b72661e372e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir herramientas personalizadas\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Hora Actual\",\n",
    "        func=get_current_time,\n",
    "        description=\"Útilidad para obtener la hora actual\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculadora de Suma\",\n",
    "        func=calculate_sum,\n",
    "        description=\"Suma de una lisata de números separados por comas o espacios\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e03bf38-fe53-423c-91d3-8be7935106ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el agente\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d6f6c-86d7-4f68-897b-5f60766eefbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso del agente\n",
    "print(agent.run(\"¿Qué hora es ahora\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4049a9-2e9d-44d5-ac97-2cfa851637b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso del agente sumando números\n",
    "print(agent.run(\"Suma los siguientes números: 10 20 30 40\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8507b8ce-6b57-4e3c-b666-fed2ebe46e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso del agente sumando números pero separados por coma y espacios\n",
    "print(agent.run(\"Suma los siguientes números: 10, 20, 30, 40\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f245b-442f-472d-b923-d9eb713a3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso del agente sumando números pero separados por coma SIN espacios\n",
    "print(agent.run(\"Suma los siguientes números: 10,20,30,40\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c50e81-ad47-4042-bcdf-ee93b2241303",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "Este tutorial ha cubierto los aspectos fundamentales y avanzados de LangChain con Gemini:\n",
    "1. Configuración básica y uso del modelo\n",
    "2. Trabajo con plantillas y cadenas\n",
    "3. Implementación de memoria en conversaciones\n",
    "4. Análisis de sentimiento\n",
    "5. Integración con bases de datos vectoriales\n",
    "6. Creación de agentes personalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f42ab-68ff-470c-a640-13c173494b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
